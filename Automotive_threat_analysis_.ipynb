{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR-aWgLLTEwC",
        "outputId": "b66d0840-5ca7-4c0b-e7f9-5949e72d0e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.81.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.83.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading openai-1.83.0-py3-none-any.whl (723 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m723.4/723.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, dnspython, aiofiles, starlette, pymongo, safehttpx, openai, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.81.0\n",
            "    Uninstalling openai-1.81.0:\n",
            "      Successfully uninstalled openai-1.81.0\n",
            "Successfully installed aiofiles-24.1.0 dnspython-2.7.0 fastapi-0.115.12 ffmpy-0.6.0 gradio-5.32.1 gradio-client-1.10.2 groovy-0.1.2 openai-1.83.0 pydub-0.25.1 pymongo-4.13.0 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai gradio pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymongo certifi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72GJ853yTViL",
        "outputId": "580136f2-60e4-4b05-9dbd-9cdff0903c85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.13.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.4.26)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM-SETUP"
      ],
      "metadata": {
        "id": "_rHmzSQ3Tbx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Your API key\n",
        "api_key =   # Replace with your OpenAI key\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Function to query ChatGPT\n",
        "def query_chatgpt(prompt, model=\"gpt-4\", temperature=0.7):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "prompt = f\"\"\"what is can bus \"\"\"\n",
        "print(query_chatgpt(prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM0BEhUDTa_n",
        "outputId": "6e3fc329-27a1-482a-ea6e-52773709a6e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAN bus, which stands for Controller Area Network bus, is a communication standard that allows microcontrollers and devices to communicate with each other within a vehicle without a host computer. It was developed by Bosch in the 1980s and is primarily used in automotive and industrial applications for connection and communication between different control systems. It's known for its robustness, efficiency, and flexibility, allowing electronic components to communicate in a manner that reduces wiring and complexity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code is for prompt checking and can use to generate attack tree"
      ],
      "metadata": {
        "id": "t-RiTQ-sTkzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "# API & DB Config\n",
        "OPENAI_API_KEY = #replace with api key\n",
        "MONGODB_URI =  #replace with uri\n",
        "\n",
        "client_ai = OpenAI(api_key=OPENAI_API_KEY)\n",
        "mongo_client = MongoClient(MONGODB_URI)\n",
        "db = mongo_client[\"threat_db\"]\n",
        "collection = db[\"attack_trees\"]\n",
        "\n",
        "EXPORT_DIR = \"csv_exports\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "def parse_mermaid_to_named_edges(mermaid_code):\n",
        "    node_labels = {}\n",
        "    edges = []\n",
        "\n",
        "    lines = mermaid_code.splitlines()\n",
        "    for line in lines:\n",
        "        node_match = re.findall(r'(\\w+)\\[(.+?)\\]', line)\n",
        "        for node_id, label in node_match:\n",
        "            node_labels[node_id.strip()] = label.strip()\n",
        "\n",
        "    edge_pattern = re.compile(r'(\\w+)\\s*-->\\s*(\\w+)')\n",
        "    for line in lines:\n",
        "        match = edge_pattern.search(line)\n",
        "        if match:\n",
        "            parent_id = match.group(1).strip()\n",
        "            child_id = match.group(2).strip()\n",
        "            parent_label = node_labels.get(parent_id, parent_id)\n",
        "            child_label = node_labels.get(child_id, child_id)\n",
        "            edges.append((parent_label, child_label))\n",
        "\n",
        "    return edges\n",
        "\n",
        "def build_ordered_paths(edges):\n",
        "    tree = defaultdict(list)\n",
        "    indegree = defaultdict(int)\n",
        "    for parent, child in edges:\n",
        "        tree[parent].append(child)\n",
        "        indegree[child] += 1\n",
        "\n",
        "    roots = set(tree.keys()) - set(indegree.keys())\n",
        "    if not roots:\n",
        "        return []\n",
        "\n",
        "    root = list(roots)[0]  # Choose first root\n",
        "    paths = []\n",
        "    queue = deque([(root, [root])])\n",
        "\n",
        "    while queue:\n",
        "        node, path = queue.popleft()\n",
        "        if node not in tree:\n",
        "            paths.append(path)\n",
        "        else:\n",
        "            for child in tree[node]:\n",
        "                queue.append((child, path + [child]))\n",
        "\n",
        "    return paths\n",
        "\n",
        "def export_structured_csv(prompt, paths):\n",
        "    filename = f\"{prompt[:30].replace(' ', '_').replace('/', '_')}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    filepath = os.path.join(EXPORT_DIR, filename)\n",
        "\n",
        "    with open(filepath, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"])\n",
        "\n",
        "        for path in paths:\n",
        "            # Fill to 4 levels max: Surface Goal, Vector, Technique, Method\n",
        "            row = path[:4] + [\" > \".join(path)]\n",
        "            while len(row) < 5:\n",
        "                row.insert(len(row)-1, \"\")\n",
        "            writer.writerow(row)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def read_csv_as_dataframe(csv_path):\n",
        "    try:\n",
        "        return pd.read_csv(csv_path)\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"])\n",
        "\n",
        "def generate_attack_tree(prompt):\n",
        "    if not prompt.strip():\n",
        "        return \"âŒ Please enter a valid prompt\", pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        system_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a cybersecurity expert. For any input threat scenario, respond with only the attack tree in valid Mermaid syntax using 'graph TD'. \"\n",
        "                \"Do not include any explanation, just output:\\n```mermaid\\ngraph TD\\n...\\n```\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        response = client_ai.chat.completions.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[system_message, {\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        output = response.choices[0].message.content.strip()\n",
        "        if output.startswith(\"```mermaid\"):\n",
        "            output = output.replace(\"```mermaid\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        collection.update_one(\n",
        "            {\"prompt\": prompt},\n",
        "            {\"$set\": {\"mermaid_code\": output, \"updated_at\": datetime.utcnow()}},\n",
        "            upsert=True\n",
        "        )\n",
        "\n",
        "        edges = parse_mermaid_to_named_edges(output)\n",
        "        paths = build_ordered_paths(edges)\n",
        "        csv_path = export_structured_csv(prompt, paths)\n",
        "        df = read_csv_as_dataframe(csv_path)\n",
        "\n",
        "        return f\"```mermaid\\n{output}\\n```\", df\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\", pd.DataFrame()\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ğŸš— Threat Tree Generator for Vehicle ECUs\")\n",
        "    prompt_input = gr.Textbox(label=\"ğŸ“ Enter Threat Prompt\", lines=5, placeholder=\"e.g. Generate attack tree for CAN bus\")\n",
        "    mermaid_output = gr.Markdown(label=\"ğŸ“Œ Mermaid Diagram\")\n",
        "    csv_table = gr.Dataframe(headers=[\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"], datatype=[\"str\"]*5, interactive=False)\n",
        "\n",
        "    generate_button = gr.Button(\"ğŸš€ Generate & Save\")\n",
        "    generate_button.click(fn=generate_attack_tree, inputs=prompt_input, outputs=[mermaid_output, csv_table])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "F9eoK5IRY0a_",
        "outputId": "5bb906cd-9137-4a7a-bebb-f3edb6ba4b82"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a4c6a10343dc9517a0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a4c6a10343dc9517a0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stored 50 attack surface prompts with label and aliases"
      ],
      "metadata": {
        "id": "Jbz7AlnuT9NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "import certifi\n",
        "import json\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# MongoDB Atlas connection URI\n",
        "MONGODB_URI = #replace with uri\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\n",
        "    MONGODB_URI,\n",
        "    tls=True,\n",
        "    tlsCAFile=certifi.where()\n",
        ")\n",
        "\n",
        "# Select database and collection\n",
        "db = client[\"threat_db\"]\n",
        "prompt_library = db[\"prompt_library\"]\n",
        "\n",
        "# 1. Predefined prompt templates with aliases\n",
        "predefined_prompts = [\n",
        "    {\n",
        "        \"label\": \"can bus injection\",\n",
        "        \"aliases\": [\"can bus\", \"controller area network\", \"can bus attack\", \"bus injection\"],\n",
        "        \"prompt\": (\n",
        "            \"\"\"Generate an attack tree for the Controller Area Network (CAN) bus in an automotive system. The root node should represent a successful attack on the CAN bus.\n",
        "\n",
        "The first level of branches should include:\n",
        "- 'Message Injection',\n",
        "- 'Denial of Service',\n",
        "- 'Spoofing',\n",
        "- 'Eavesdropping',\n",
        "- 'Fault Injection',\n",
        "- 'Firmware Manipulation',\n",
        "- 'Physical Access'.\n",
        "\n",
        "Use Mermaid format with 'graph TD' syntax.\n",
        "\n",
        "For each branch, expand with at least two sub-branches on the second level, and for each of those sub-branches, add at least two further sub-branches on the third level, resulting in a three-level hierarchy in total.\n",
        "\n",
        "Specifically:\n",
        "\n",
        "- For 'Message Injection', include sub-branches 'Replay Attack' and 'Arbitrary Message Injection'. Then expand 'Replay Attack' with two sub-branches: 'Capturing Packets' and 'Resending Packets'. Expand 'Arbitrary Message Injection' with 'Crafting Messages' and 'Injecting Malicious Commands'.\n",
        "\n",
        "- For 'Denial of Service', include 'Bus Flooding' and 'Error Frame Injection'. Expand 'Bus Flooding' with 'Continuous Message Sending' and 'Resource Exhaustion'. Expand 'Error Frame Injection' with 'Error Frame Flood' and 'Bus Off State'.\n",
        "\n",
        "- For 'Spoofing', include 'ID Spoofing' and 'Timing Spoofing'. Expand 'ID Spoofing' with 'Forged IDs' and 'Masquerading'. Expand 'Timing Spoofing' with 'Delay Injection' and 'Replay Timing Manipulation'.\n",
        "\n",
        "- For 'Eavesdropping', include 'Passive Listening' and 'Data Capture'. Expand 'Passive Listening' with 'Bus Monitoring' and 'Signal Interception'. Expand 'Data Capture' with 'Message Logging' and 'Packet Analysis'.\n",
        "\n",
        "- For 'Fault Injection', include 'Voltage Manipulation' and 'Clock Glitching'. Expand 'Voltage Manipulation' with 'Power Supply Interruption' and 'Voltage Spike'. Expand 'Clock Glitching' with 'Clock Signal Interference' and 'Timing Violation'.\n",
        "\n",
        "- For 'Firmware Manipulation', include 'Malicious Firmware Update' and 'Firmware Downgrade'. Expand 'Malicious Firmware Update' with 'Tampered Firmware File' and 'OTA Exploitation'. Expand 'Firmware Downgrade' with 'Rollback Exploit' and 'Signature Bypass'.\n",
        "\n",
        "- For 'Physical Access', include 'OBD-II Port Exploit' and 'ECU Extraction'. Expand 'OBD-II Port Exploit' with 'Sniffing Traffic' and 'Sending Commands'. Expand 'ECU Extraction' with 'Direct Flash Access' and 'Hardware Debugging Interface'.\n",
        "\n",
        "Make sure:\n",
        "- All node labels are short and readable (no long sentences)\n",
        "- Use unique node IDs or Mermaid-friendly naming (avoid duplicate text nodes)\n",
        "- No overlapping concepts\n",
        "- Output is valid Mermaid syntax\n",
        "\n",
        "Output the full attack tree **only** in valid Mermaid code, wrapped in triple backticks like this:\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "\"\"\"\n",
        "\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "# 2. Load additional prompts from a local JSON file (if provided)\n",
        "def load_prompts_from_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "            else:\n",
        "                print(\"âš ï¸ JSON file must contain a list of prompts.\")\n",
        "                return []\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading JSON file: {e}\")\n",
        "        return []\n",
        "\n",
        "# 3. Merge predefined prompts with JSON prompts\n",
        "json_prompts = load_prompts_from_file(\"ATT2.json\")  # Update path if needed\n",
        "all_prompts = predefined_prompts + json_prompts\n",
        "\n",
        "# 4. Insert or update all prompts\n",
        "for item in all_prompts:\n",
        "    if \"label\" in item and \"prompt\" in item:\n",
        "        prompt_library.update_one(\n",
        "            {\"label\": item[\"label\"]},\n",
        "            {\"$set\": item},\n",
        "            upsert=True\n",
        "        )\n",
        "    else:\n",
        "        print(f\"âš ï¸ Skipping invalid prompt entry: {item}\")\n",
        "\n",
        "print(\"âœ… Prompt library with aliases populated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmpPCtqWTuQa",
        "outputId": "bc3bf38e-3770-42e5-9313-b116836d1f3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Prompt library with aliases populated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ui+created library in csv form"
      ],
      "metadata": {
        "id": "op12uaoKUfGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# ğŸ“¦ Library Imports\n",
        "# ========================\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "# ========================\n",
        "# ğŸ” API & DB Config\n",
        "# ========================\n",
        "OPENAI_API_KEY = #replace with api key\n",
        "MONGODB_URI =  #replace with uri\n",
        "\n",
        "client_ai = OpenAI(api_key=OPENAI_API_KEY)\n",
        "mongo_client = MongoClient(MONGODB_URI)\n",
        "db = mongo_client[\"threat_db\"]\n",
        "attack_tree_collection = db[\"attack_trees\"]\n",
        "prompt_library = db[\"prompt_library\"]\n",
        "\n",
        "EXPORT_DIR = \"csv_exports\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# ========================\n",
        "# ğŸŸ¢ Updated Parsing + CSV Logic from second code snippet\n",
        "# ========================\n",
        "\n",
        "def parse_mermaid_to_named_edges(mermaid_code):\n",
        "    node_labels = {}\n",
        "    edges = []\n",
        "\n",
        "    lines = mermaid_code.splitlines()\n",
        "    for line in lines:\n",
        "        node_match = re.findall(r'(\\w+)\\[(.+?)\\]', line)\n",
        "        for node_id, label in node_match:\n",
        "            node_labels[node_id.strip()] = label.strip()\n",
        "\n",
        "    edge_pattern = re.compile(r'(\\w+)\\s*-->\\s*(\\w+)')\n",
        "    for line in lines:\n",
        "        match = edge_pattern.search(line)\n",
        "        if match:\n",
        "            parent_id = match.group(1).strip()\n",
        "            child_id = match.group(2).strip()\n",
        "            parent_label = node_labels.get(parent_id, parent_id)\n",
        "            child_label = node_labels.get(child_id, child_id)\n",
        "            edges.append((parent_label, child_label))\n",
        "\n",
        "    return edges\n",
        "\n",
        "def build_ordered_paths(edges):\n",
        "    tree = defaultdict(list)\n",
        "    indegree = defaultdict(int)\n",
        "    for parent, child in edges:\n",
        "        tree[parent].append(child)\n",
        "        indegree[child] += 1\n",
        "\n",
        "    roots = set(tree.keys()) - set(indegree.keys())\n",
        "    if not roots:\n",
        "        return []\n",
        "\n",
        "    root = list(roots)[0]  # Take first root\n",
        "    paths = []\n",
        "    queue = deque([(root, [root])])\n",
        "\n",
        "    while queue:\n",
        "        node, path = queue.popleft()\n",
        "        if node not in tree:\n",
        "            paths.append(path)\n",
        "        else:\n",
        "            for child in tree[node]:\n",
        "                queue.append((child, path + [child]))\n",
        "\n",
        "    return paths\n",
        "\n",
        "def export_structured_csv(label, paths):\n",
        "    safe_label = label[:30].replace(' ', '_').replace('/', '_')\n",
        "    filename = f\"{safe_label}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    filepath = os.path.join(EXPORT_DIR, filename)\n",
        "\n",
        "    with open(filepath, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"])\n",
        "\n",
        "        for path in paths:\n",
        "            # Fill max 4 columns, pad if less\n",
        "            row = path[:4] + [\" > \".join(path)]\n",
        "            while len(row) < 5:\n",
        "                row.insert(len(row) - 1, \"\")\n",
        "            writer.writerow(row)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def read_csv_as_dataframe(filepath):\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        df.drop_duplicates(subset=[\"Path\"], inplace=True)\n",
        "        return df\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"])\n",
        "\n",
        "# ========================\n",
        "# Updated load_saved_attack_tree for Tab 2 using new logic\n",
        "# ========================\n",
        "\n",
        "def load_saved_attack_tree(label):\n",
        "    if not label:\n",
        "        return \"âŒ No label provided.\", pd.DataFrame(), None\n",
        "\n",
        "    doc = attack_tree_collection.find_one({\"label\": label})\n",
        "\n",
        "    if not doc:\n",
        "        # Try alias lookup in prompt_library\n",
        "        alias_doc = prompt_library.find_one({\"aliases\": {\"$in\": [label.lower()]}})\n",
        "        if alias_doc:\n",
        "            canonical_label = alias_doc[\"label\"]\n",
        "            doc = attack_tree_collection.find_one({\"label\": canonical_label})\n",
        "\n",
        "    if not doc or \"mermaid_code\" not in doc:\n",
        "        return \"âŒ No stored attack tree found.\", pd.DataFrame(), None\n",
        "\n",
        "    mermaid_code = doc[\"mermaid_code\"]\n",
        "    edges = parse_mermaid_to_named_edges(mermaid_code)\n",
        "    paths = build_ordered_paths(edges)\n",
        "    csv_path = export_structured_csv(doc[\"label\"], paths)\n",
        "    df = read_csv_as_dataframe(csv_path)\n",
        "\n",
        "    return f\"```mermaid\\n{mermaid_code}\\n```\", df, csv_path\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ğŸ¤– Generate Tree from Prompt (Unchanged, original first tab logic)\n",
        "# ========================\n",
        "\n",
        "def generate_attack_tree_from_label(label_selected):\n",
        "    if not label_selected:\n",
        "        return \"âŒ Select a threat scenario.\"\n",
        "\n",
        "    # Try exact match on label\n",
        "    doc = prompt_library.find_one({\"label\": label_selected})\n",
        "\n",
        "    # If not found, try alias match (case-insensitive)\n",
        "    if not doc:\n",
        "        doc = prompt_library.find_one({\"aliases\": {\"$in\": [label_selected.lower()]}})\n",
        "\n",
        "    if not doc or \"prompt\" not in doc:\n",
        "        return f\"âŒ No prompt or alias found for '{label_selected}'\"\n",
        "\n",
        "    matched_prompt = doc[\"prompt\"]\n",
        "    label_to_save = doc[\"label\"]  # Use canonical label for saving\n",
        "\n",
        "    try:\n",
        "        system_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a cybersecurity expert. Return only the attack tree in Mermaid format using:\\nmermaid\\ngraph TD\\n...\\nStructure the tree logically using OR/AND logic nodes and hierarchical breakdown of threats starting from surface goal > attack vector > attack method.\"\n",
        "        }\n",
        "\n",
        "        response = client_ai.chat.completions.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[system_message, {\"role\": \"user\", \"content\": matched_prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        mermaid_code = response.choices[0].message.content.strip()\n",
        "        if mermaid_code.startswith(\"mermaid\"):\n",
        "            mermaid_code = mermaid_code.replace(\"mermaid\", \"\").strip()\n",
        "\n",
        "        attack_tree_collection.update_one(\n",
        "            {\"label\": label_to_save},\n",
        "            {\"$set\": {\n",
        "                \"prompt\": matched_prompt,\n",
        "                \"mermaid_code\": mermaid_code,\n",
        "                \"updated_at\": datetime.utcnow()\n",
        "            }},\n",
        "            upsert=True\n",
        "        )\n",
        "\n",
        "        return f\"mermaid\\n{mermaid_code}\\n\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ğŸŒ Gradio UI with two tabs\n",
        "# ========================\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"ğŸ§  Generate Attack Tree\"):\n",
        "        gr.Markdown(\"### ğŸ” attack tree\")\n",
        "\n",
        "        label_dropdown = gr.Dropdown(\n",
        "            choices=sorted([doc[\"label\"] for doc in prompt_library.find({}, {\"label\": 1, \"_id\": 0}) if \"label\" in doc]),\n",
        "            label=\"ğŸ“Œ Select or Type\",\n",
        "            interactive=True,\n",
        "            allow_custom_value=True\n",
        "        )\n",
        "        generate_button = gr.Button(\"ğŸš€ Generate Attack Tree\")\n",
        "        mermaid_display = gr.Markdown(label=\"ğŸ“ˆ Generated Attack Tree\")\n",
        "\n",
        "        generate_button.click(\n",
        "            fn=generate_attack_tree_from_label,\n",
        "            inputs=label_dropdown,\n",
        "            outputs=mermaid_display\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"ğŸ“‚ Library\"):\n",
        "        gr.Markdown(\"### ğŸ“‰ View and Export Structured Threat Trees\")\n",
        "\n",
        "        saved_dropdown = gr.Dropdown(\n",
        "            choices=sorted(set([doc[\"label\"] for doc in attack_tree_collection.find({}, {\"label\": 1, \"_id\": 0}) if \"label\" in doc])),\n",
        "            label=\"ğŸ“Œ Select or Type Stored Tree\",\n",
        "            interactive=True,\n",
        "            allow_custom_value=True\n",
        "        )\n",
        "        mermaid_output = gr.Markdown(label=\"ğŸ“ˆ Saved Attack Tree\")\n",
        "        relation_table = gr.Dataframe(headers=[\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"], datatype=[\"str\"]*5, interactive=False)\n",
        "        download_button = gr.File(label=\"ğŸ“¥ Download CSV\")\n",
        "        regen_button = gr.Button(\"ğŸ”„ Regenerate Tree from Prompt\")\n",
        "\n",
        "        def wrapper_load(label):\n",
        "            if not label:\n",
        "                return \"âŒ Select a saved attack tree.\", pd.DataFrame(columns=[\"Surface Goal\", \"Attack Vector\", \"Technique\", \"Method\", \"Path\"]), None\n",
        "            mermaid, df, csv_path = load_saved_attack_tree(label)\n",
        "            return mermaid, df, csv_path\n",
        "\n",
        "        saved_dropdown.change(\n",
        "            fn=wrapper_load,\n",
        "            inputs=saved_dropdown,\n",
        "            outputs=[mermaid_output, relation_table, download_button]\n",
        "        )\n",
        "\n",
        "        regen_button.click(\n",
        "            fn=generate_attack_tree_from_label,\n",
        "            inputs=saved_dropdown,\n",
        "            outputs=mermaid_output\n",
        "        )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "LcMyDV_-dwpO",
        "outputId": "b466626c-38e7-406c-d835-991211212e31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4f8b42c1a28812c015.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4f8b42c1a28812c015.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}